%http://compneuro.uwaterloo.ca/research/software/part-four-feedback-and-dynamics.html
%http://compneuro.uwaterloo.ca/research.html
%http://compneuro.uwaterloo.ca/research/constants-constraints/neurotransmitter-time-constants-pscs.html

@MISC{EDVACreport1945,
author = {John~von~Neumann},
title = {{First Draft of a Report on the EDVAC}},
year = {1945},
howpublished = {
\url{http://www.wiley.com/legacy/wileychi/wang_archi/supp/appendix_a.pdf}
}
}


@BOOK{SystemCBook2010,
	author = {David C. Black and Jack Donovan and Bill Bunton and Anna Keist},
	title = {SystemC: From the Ground Up},
	year = {2010},
	publisher = {Springer},
	edition = {second},
	place = {New York Dordrecht Heidelberg London},
	isbn = {978-0-387-69957-8}
}

@phdthesis{borph2000,
	author = {Hayden Kwok-Hay So},
	title = {{BORPH: An Operating System for FPGA-Based Reconfigurable Computers}},
	school = {University of California, Berkeley},
	year = {2000},
}

@CONFERENCE{AmdahlSingleProcessor67,
	author= {{Amdahl, G. M.}},
	year = {1967},
	title = {{Validity of the Single Processor Approach to Achieving Large-Scale Computing Capabilities}},
	booktitle = {AFIPS Conference Proceedings},
	volume = {30},
	pages = {483-485},
	doi = {doi:10.1145/1465482.1465560}
}
%note = {\url{http://www-inst.eecs.berkeley.edu/~n252/paper/Amdahl.pdf}},

%article An article from a journal or magazine. Required elds: author, title,
%journal, year. Optional elds: volume, number, pages, month, note.
@article{YavitsMulticoreAmdahl2014,
	author = { Yavits, L. and  Morad, A. and Ginosar,R.  },
	title = {{The effect of communication and synchronization on Amdahl’s law in multicore systems}},
	journal = {Parallel Computing},
	volume = {40},
	year = {2014},
	pages ={1-16},
	number = {1}
}


@book{QTProgramming,
author={Blanchette, Jasmin and Summerfield, Mark},
year = {2006},
title = {{A Brief History of Qt. C++ GUI Programming with Qt 4}},
publisher = {Prentice-Hall}
}

@book{HennessyArchitecture2007,
	author = {John L. Hennessy and  David A. Patterson},
	title = {{Computer Architecture: A Quantitative Approach}}, 
	publisher = {Morgan Kaufmann Publishers},
	isbn = {978-0-12-370490-0},
	year = {2007}
}

@CONFERENCE{SynchronizationEverything2013,
  	author = {David,	T.  and	Guerraoui, R. and	Trigonakis, V.},
  	title = {{Everything you always wanted to know about synchronization but were afraid to ask}},
  	booktitle = {Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles (SOSP '13)},
  	year = {2013},
  	pages = {33-48},
  	doi = {10.1145/2517349.2522714}
  }


@INPROCEEDINGS{Zhao2003,
  author = {Jianjun Zhao},
  title = {Data-flow-based unit testing of aspect-oriented programs},
  booktitle = {Proceedings of the 27th Annual International Computer Software and Applications Conference (COMPSAC 2003)},
  year = {2003},
  pages = {188--197},
  publisher = {IEEE Computer Society},
  doi = {http://ieeexplore.ieee.org/iel5/8813/27898/01245340.pdf?isnumber=27898\&prod=STD\&arnumber=1245340\&arnumber=1245340\&arSt=+188\&ared=+197\&arAuthor=Jianjun+Zhao},
  location = {Dallas, USA},
}

@incollection{InvasiveComputing:2011,
	author = {J.Teich and J. Henkel and A. Herkersdorf and D. Schmitt-Landsiedel and W. Schr\"oder-Preikschat and G. Snelting},
	booktitle   = {Multiprocessor System-on-Chip},
	title = {{Invasive Computing: An Overview}},
		editor      = {M. H\"ubner and		J. Becker},
	year = {2011},
	isbn = {978-1-4419-6459-5},
	pages = {241-268},
	publisher   = {Springer},
}

@article{Pingali:2011:TaoOfParallelism,
	author = {Pingali, Keshav and Nguyen, Donald and Kulkarni, Milind and Burtscher, Martin and Hassaan, M. Amber and Kaleem, Rashid and Lee, Tsung-Hsien and Lenharth, Andrew and Manevich, Roman and M{\'e}ndez-Lojo, Mario and Prountzos, Dimitrios and Sui, Xin},
	title = {{The Tao of Parallelism in Algorithms}},
	journal = {SIGPLAN Not.},
	issue_date = {June 2011},
	volume = {46},
	number = {6},
	month = jun,
	year = {2011},
	issn = {0362-1340},
	pages = {12--25},
	numpages = {14},
	url = {http://doi.acm.org/10.1145/1993316.1993501},
	doi = {10.1145/1993316.1993501},
	acmid = {1993501},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {amorphous data-parallelism, galois system, irregular programs, operator formulation, tao-analysis},
} 

%	howpublished = {\url{http://invasic.informatik.uni-erlangen.de/en/index.php}}

@article{DarkSilicon2012,
author = {Esmaeilzadeh, H. and  Blem, E. and St. Amant, R. and Sankaralingam, K. and et al.},
title = {{Dark Silicon and the End of Multicore Scaling}},
journal = {IEEE Micro},
volume = {32},
year = {2012},
pages = {122-134},
number = {3}
}

@CONFERENCE{ClockVsIPC2000,
author = {Vikas Agarwal and M.S. Hrishikesh and StephenW. Keckler and Doug Burger},
title = {{Clock Rate versus IPC: The End of the Road for Conventional Microarchitectures}},
booktitle = {Proceedings of the 27th Annual International Symposium on Computer Architecture},
year = {2000},
howpublished = {\url{www.cs.utexas.edu/~skeckler/pubs/isca00.pdf}}
}

@article{EPIC:2000,
	author = {Schlansker, M.S. and Rau, B.R.},
	title = {{EPIC: Explicitly Parallel Instruction Computing}},
	journal = {Computer},
	issue_date = {February 2000},
	volume = {33},
	number = {2},
	month = feb,
	year = {2000},
	pages = {37--45},
	doi = {10.1109/2.820037},
	publisher = {IEEE}
} 

@article{ReliableParallel2014,
	author = {Junfeng Yang and Heming Cui and Jingyue Wu and Yang Tang and Gang Hu},
	title = {{Making Parallel Programs Reliable with Stable Multithreading}},
	journal = {Communications of the ACM},
	volume = {57},
	year = {2014},
	pages = {58-69},
	number = {3},
	doi = {10.1145/2500875}
}
@ARTICLE{Viskin:ViewpointProgrammingMulticore,
	AUTHOR = "Uzi Vishkin",
	TITLE = "{Is Multicore Hardware for	General-Purpose Parallel Processing Broken?}",
	JOURNAL = "Communications of the ACM",
	VOLUME = {57},
	NUMBER = {4},
	PAGES = {35},
	MONTH = "May",
	DOI = {10.1145/2580945},
	YEAR = {2014}	}
	
@ARTICLE{Larus:ProgrammingMulticoreCACM,
	AUTHOR = "James Larus",
	TITLE = "{Programming Multicore Computers}",
	JOURNAL = "Communications of the ACM",
	VOLUME = {58},
	NUMBER = {5},
	PAGES = {76},
	MONTH = "May",
	DOI = {10.1145/2580945},
	YEAR = {2015}	}

@article{NinjaPerformanceGap:2015:CACM,
	author = {Satish, Nadathur and Kim, Changkyu and Chhugani, Jatin and Saito, Hideki and Krishnaiyer, Rakesh and Smelyanskiy, Mikhail and Girkar, Milind and Dubey, Pradeep},
	title = "{Can Traditional Programming Bridge the Ninja Performance Gap for Parallel Computing Applications?}",
	journal = {Commun. ACM},
	issue_date = {May 2015},
	volume = {58},
	number = {5},
	month = apr,
	year = {2015},
	issn = {0001-0782},
	pages = {77--86},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/2742910},
	doi = {10.1145/2742910},
	acmid = {2742910},
	publisher = {ACM},
	address = {New York, NY, USA},
}


@article{HillMulticoreAmdahl2008,
	author = {Hill, M. D.  and Marty, M. R. },
	title = {{Amdahl's Law in the Multicore Era}},
	journal = {IEEE Computer },
	volume = {41},
	year = {2008},
	pages ={33-38},
	number = {7}
}

@book{hallaron,
author = {Bryant, Randal E. and O'Hallaron, David R.},
 title = {Computer Systems: A Programmer's Perspective},
 year = {2014},
 isbn = {0136108040, 9780136108047},
 edition = {2nd},
 publisher = {Addison-Wesley Publishing Company},
 address = {USA},
}


@article{HWcontrolledthreadsMahesri:2007,
	author = {Mahesri, Aqeel and Wang, Nicholas J. and Patel, Sanjay J.},
	title = {{Hardware Support for Software Controlled Multithreading}},
	journal = {SIGARCH Comput. Archit. News},
	issue_date = {March 2007},
	volume = {35},
	number = {1},
	month = mar,
	year = {2007},
	issn = {0163-5964},
	pages = {3--12},
	numpages = {10},
	doi = {10.1145/1241601.1241606},
	acmid = {1241606},
	publisher = {ACM},
	address = {New York, NY, USA},
} 


@inbook{vonNeumannOrigins,
	author = {Aspray, W.},
	editor = {Bernard Cohen and William Aspray},
	title = {{John von Neumann and the Origins of Modern Computing}}, 
	publisher = { MIT Press, Cambridge},
	pages = {34--48},
	year = {1990},
}


@techreport{EDVACEckertMauchly,
author = {J. P. Eckert, Jr. and J. W. Mauchly},
title = {{Automatic High-Speed Computing: A Progress Report on the EDVAC}},
institution = {Moore School Library, University of Pennsylvania, Philadephia},
year = {1945},
month = {September},
number = {Report of Work under Contract No. W-670-ORD-4926, Supplement No 4}
}


@article{FateofEDVAC1993,
 author = {Williams, Michael R.},
 title = {{The Origins, Uses, and Fate of the EDVAC}},
 journal = {IEEE Ann. Hist. Comput.},
 issue_date = {January 1993},
 volume = {15},
 number = {1},
 month = jan,
 year = {1993},
 issn = {1058-6180},
 pages = {22--38},
 numpages = {17},
 url = {http://dx.doi.org/10.1109/85.194089},
 doi = {10.1109/85.194089},
 acmid = {612518},
 publisher = {IEEE Educational Activities Department},
 address = {Piscataway, NJ, USA},
} 


@MISC{CharmIntroduction93,
	author = {L. V. Kale and S. Krisnan},
	title = {{CHARM++: A Portable Concurrent Object Oriented System Bases on C++}},
	year = {1993},
	howpublished = {\url{http://http://charm.cs.illinois.edu/newPapers/93-02/paper.pdf}}
}


@CONFERENCE{ComputingDensity2008,
author = {J.Williams and A. D. George and J. Richardson and K. Gosrani and S. Suresh},
title = {Computational density of fixed and reconfigurable multi-core devices for application acceleration},
booktitle = {Proceedings of Reconfigurable Systems Summer Institute, Urbana, IL, Jul. 2008.},
year = {2008}
}


@inproceedings{IannucciIssues:1988,
	author = {Arvind and Iannucci, Robert A.},
	title = {Two Fundamental Issues in Multiprocessing},
	booktitle = {4th International DFVLR Seminar on Foundations of Engineering Sciences on Parallel Computing in Science and Engineering},
	year = {1988},
	isbn = {0-387-18923-8},
	location = {Bonn, Germany},
	pages = {61--88},
	numpages = {28},
	acmid = {52802},
	publisher = {Springer-Verlag New York, Inc.},
	address = {New York, NY, USA},
} 


@MISC{WallLimitsOfILP:1993,
author = {David~W.~Wall},
title = {{Limits of Instruction-Level Parallelism}},
year = {1993},
howpublished = {\url{http://www.hpl.hp.com/techreports/Compaq-DEC/WRL-93-6.pdf}}
}


@article{NicolauFischer1984,
author = {Alexandru Nicolau and Joseph A. Fisher},
title = {{Measuring the parallelism available for very long instruction word architectures}},
journal = {IEEE Transactions on Computers},
volume = {C-33},
year = {1984},
number = {11},
pages = {968-976}
}


@article{BackusNeumannProgrammingStyle,
author = {J. Backus},
title = {{Can Programming Languages Be liberated from the von Neumann Style? A Functional Style and its Algebra of Programs}},
journal = {Communications of the ACM},
volume = {21},
year = {1978},
pages = {613–-641}
}
@ARTICLE{HennesseyParallelCACM2009,
	AUTHOR = "Krste Asanovic and Rastislav Bodik and James Demmel and Tony
	Keaveny and Kurt Keutzer and John Kubiatowicz and Nelson Morgan
	and David Patterson and Koushik Sen and John Wawrzynek and
	David Wessel and Katherine Yelick",
	TITLE = "{A View of the Parallel Computing Landscape}",
	JOURNAL = "Communications of the ACM",
	VOLUME = {52},
	NUMBER = {10},
	PAGES = {56-67},
	YEAR = {2009}	}


@inproceedings{InefficiencyHameed2010,
 author = {Hameed, Rehan and Qadeer, Wajahat and Wachs, Megan and Azizi, Omid and Solomatnikov, Alex and Lee, Benjamin C. and Richardson, Stephen and Kozyrakis, Christos and Horowitz, Mark},
 title = {Understanding Sources of Inefficiency in General-purpose Chips},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 series = {ISCA '10},
 year = {2010},
 isbn = {978-1-4503-0053-7},
 location = {Saint-Malo, France},
 pages = {37--47},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1815961.1815968},
 doi = {10.1145/1815961.1815968},
 acmid = {1815968},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ASIC, chip multiprocessor, customization, energy efficiency, h.264, high performance, tensilica},
} 


@article{ReconfigurableMulticoresWilliams:2010,
 author = {Williams, Jason and Massie, Chris and George, Alan D. and Richardson, Justin and Gosrani, Kunal and Lam, Herman},
 title = {Characterization of Fixed and Reconfigurable Multi-Core Devices for Application Acceleration},
 journal = {ACM Trans. Reconfigurable Technol. Syst.},
 issue_date = {November 2010},
 volume = {3},
 number = {4},
 month = nov,
 year = {2010},
 issn = {1936-7406},
 pages = {19:1--19:29},
 articleno = {19},
 numpages = {29},
 url = {http://doi.acm.org/10.1145/1862648.1862649},
 doi = {10.1145/1862648.1862649},
 acmid = {1862649},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Computational density per watt, internal memory bandwidth},
} 


@phdthesis{HSWscalable2012,
	author = {Daniel Sanchez Martin},
	title = {{HARDWARE AND SOFTWARE TECHNIQUES FOR SCALABLE
	THOUSAND-CORE SYSTEMS}},
	school = {Stanford University, Berkeley},
	year = {2012},	 
	howpublished = {\url{http://purl.stanford.edu/mz572jk7876}}
}

@techreport{Adaptiveresourcecontrolinmulticoresystems2013,
author = {F. Xia and A. Mokhov and A. Yakovlev and A. Iliasov and A. Rafiev and A. Romanovsky},
title = {{Adaptive resource control in multi-core systems}},
institution = {Newcastle University, School of Electrical and Electronic Engineering, Philadephia},
year = {2013},
number = {NCL-EEE-MICRO-TR-2013-183},
howpublished = {\url{http://async.org.uk/tech-reports/NCL-EEE-MICRO-TR-2013-183.pdf}}
}


@MISC{Intel10GHz:2014,
author = {Intel},
title = {{Why has CPU frequency ceased to grow?}},
year = {2014},
url = {https://software.intel.com/en-us/blogs/2014/02/19/why-has-cpu-frequency-ceased-to-grow}
}


@MISC{TorwaldsParallel2014,
author = {Torvalds, Linus},
title = {{Linus: The whole "parallel computing is the future" is a bunch of crock}},
year = {2014},
howpublished = {\url{http://highscalability.com/blog/2014/12/31/linus-the-whole-parallel-computing-is-the-future-is-a-bunch.html}}
}

@MISC{Cypress15,
        author = {Cypress},
        title = {{CY7C026A: 16K x 16 Dual-Port Static RAM}},
        year = {2015},
        howpublished = {\url{http://www.cypress.com/documentation/datasheets/cy7c026a-16k-x-16-dual-port-static-ram}}
}

@article{NinjaPerformanceGap:2015:CACM,
	author = {Satish, Nadathur and Kim, Changkyu and Chhugani, Jatin and Saito, Hideki and Krishnaiyer, Rakesh and Smelyanskiy, Mikhail and Girkar, Milind and Dubey, Pradeep},
	title = "{Can Traditional Programming Bridge the Ninja Performance Gap for Parallel Computing Applications?}",
	journal = {Commun. ACM},
	issue_date = {May 2015},
	volume = {58},
	number = {5},
	month = apr,
	year = {2015},
	issn = {0001-0782},
	pages = {77--86},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/2742910},
	doi = {10.1145/2742910},
	acmid = {2742910},
	publisher = {ACM},
	address = {New York, NY, USA},
} 

@book{HwangParallelism:2016,
	author = {Kai Hwang and Naresh Jotwani},
	title = {{Advanced Computer Architecture: Parallelism, Scalability, Programmability}}, 
	publisher = {Mc Graw Hill},
	isbn = {978-93-392-2093-8},
	edition = {3},
	year = {2016},
}


@Article{FuSunwaySystem2016,
author="Fu, Haohuan
and Liao, Junfeng
and Yang, Jinzhe
and Wang, Lanning
and Song, Zhenya
and Huang, Xiaomeng
and Yang, Chao
and Xue, Wei
and Liu, Fangfang
and Qiao, Fangli
and Zhao, Wei
and Yin, Xunqiang
and Hou, Chaofeng
and Zhang, Chenglong
and Ge, Wei
and Zhang, Jian
and Wang, Yangang
and Zhou, Chunbo
and Yang, Guangwen",
title={{The Sunway TaihuLight supercomputer: system and applications}},
journal="{Science China Information Sciences}",
year="2016",
volume="59",
number="7",
pages="1--16",
abstract="The Sunway TaihuLight supercomputer is the world's first system with a peak performance greater than 100 PFlops. In this paper, we provide a detailed introduction to the TaihuLight system. In contrast with other existing heterogeneous supercomputers, which include both CPU processors and PCIe-connected many-core accelerators (NVIDIA GPU or Intel Xeon Phi), the computing power of TaihuLight is provided by a homegrown many-core SW26010 CPU that includes both the management processing elements (MPEs) and computing processing elements (CPEs) in one chip. With 260 processing elements in one CPU, a single SW26010 provides a peak performance of over three TFlops. To alleviate the memory bandwidth bottleneck in most applications, each CPE comes with a scratch pad memory, which serves as a user-controlled cache. To support the parallelization of programs on the new many-core architecture, in addition to the basic C/C++ and Fortran compilers, the system provides a customized Sunway OpenACC tool that supports the OpenACC 2.0 syntax. This paper also reports our preliminary efforts on developing and optimizing applications on the TaihuLight system, focusing on key application domains, such as earth system modeling, ocean surface wave modeling, atomistic simulation, and phase-field simulation.",
issn="1869-1919",
doi="10.1007/s11432-016-5588-7",
url="http://dx.doi.org/10.1007/s11432-016-5588-7"
}

@ARTICLE{VeghSegregatedCores2016,
   author = {{V{\'e}gh}, J.},
    title = {{Why to build many-core chips from cooperating, rather than segregated cores? And HOW?}},
  journal = {ACM TOCS},
 keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, 68N15, 68N19, C.1.3, D.2.11, F.1.2},
     year = 2016,
    month = october,
    pages = {submitted}
}

@ARTICLE{Ungerer2010,
   author = {T. Ungerer et al.},
    title = {{ MERASA: Multi-core execution of hard real-time applications supporting analyzability}},
  journal = {{IEEE Micro}},
  issue = {99},
     year = 2010,
    pages = {66-75}
}

@MISC{CMake:2016,
        author = {{Kitware}},
        title = {{An open-source, cross-platform family of tools designed to build, test and package software}},
        year = {2016},
        howpublished = {\url{https://cmake.org/}}
 }

@MISC{FileSystemHierarchy:2016,
        author = {{Wikipedia}},
        title = {{Filesystem Hierarchy Standard}},
        year = {2016},
        howpublished = {\url{https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard}}
 }


@MISC{gtest:2016,
author = {Google},
title = {{Google's C++ test framework}},
year = {2016},
howpublished = {\url{https://github.com/google/googletest}}
}

@MISC{SystemCsite:2017,
author = {Accellera},
title = {{SystemC}},
year = {2017},
howpublished = {\url{http://accellera.org/downlo
@subsection THE_SOLUTION A possible solution
ads/standards/systemc}}
}

@MISC{VahidSite:2006,
author = {F. Vahid},
title = {{SystemC}},
year = {2006},
howpublished = {\url{http://www.cs.ucr.edu/~vahid/sproj/SystemCLab/}}
}

@ARTICLE{ComputingPerformance:2011,
        author = {S.~H.~Fuller and  L.~I.~Millett},
        title = {{Computing Performance: Game Over or Next Level?}},
        journal = {Computer},
        volume = {44},
        Issue = {1},
        pages = {31-38},
        year = {2011}
}

@MISC{CPPlanguage,
author = {B. Stroustrup},
title = {{C++ Glossary}},
year = {2012},
howpublished = {\url{http://www.stroustrup.com/glossary.html}}
}


@Article{CooperativeComputing2015,
author="Zheng, Fang
and Li, Hong-Liang
and Lv, Hui
and Guo, Feng
and Xu, Xiao-Hong
and Xie, Xiang-Hui",
title="Cooperative Computing Techniques for a Deeply Fused and Heterogeneous Many-Core Processor Architecture",
journal="Journal of Computer Science and Technology",
year="2015",
month="Jan",
day="01",
volume="30",
number="1",
pages="145--162",
abstract="Due to advances in semiconductor techniques, many-core processors have been widely used in high performance computing. However, many applications still cannot be carried out efficiently due to the memory wall, which has become a bottleneck in many-core processors. In this paper, we present a novel heterogeneous many-core processor architecture named deeply fused many-core (DFMC) for high performance computing systems. DFMC integrates management processing elements (MPEs) and computing processing elements (CPEs), which are heterogeneous processor cores for different application features with a unified ISA (instruction set architecture), a unified execution model, and share-memory that supports cache coherence. The DFMC processor can alleviate the memory wall problem by combining a series of cooperative computing techniques of CPEs, such as multi-pattern data stream transfer, efficient register-level communication mechanism, and fast hardware synchronization technique. These techniques are able to improve on-chip data reuse and optimize memory access performance. This paper illustrates an implementation of a full system prototype based on FPGA with four MPEs and 256 CPEs. Our experimental results show that the effect of the cooperative computing techniques of CPEs is significant, with DGEMM (double-precision matrix multiplication) achieving an efficiency of 94{\%}, FFT (fast Fourier transform) obtaining a performance of 207 GFLOPS and FDTD (finite-difference time-domain) obtaining a performance of 27 GFLOPS.",
issn="1860-4749",
doi="10.1007/s11390-015-1510-9",
url="https://doi.org/10.1007/s11390-015-1510-9"
}


@inproceedings{Mahlke:1992:LimitedRegisters,
author = {Mahlke, S.A. and Chen, W.Y. and Chang, P.P. and Hwu, W.-M.W.},
title = {Scalar program performance on multiple-instruction-issue processors with a limited number of registers},
booktitle = {Proceedings of the Twenty-Fifth Hawaii International Conference on System Sciences},
date = {7-10 Jan 1992},
year = {1992},
volume = {1},
pages = {34 - 44},
doi = {10.1109/HICSS.1992.183141},
}


@article{KiloCoreChip:2017,
	author    = {	Brent Bohnenstiehl and Aaron Stillmaker and Jon Pimentel and Timothy Andreas and Bin Liu and Anh Tran and Emmanuel Adeagbo and Bevan Baas},
	title     = {KiloCore: A Fine-Grained 1,000-Processor Array for Task-Parallel Applications
	},
	journal   = {IEEE Micro},
	volume    = {37},
	issue     = {2},
	year      = {2017},
	pages     = {63-69},
	doi       = { 10.1109/MM.2017.34},
}

   
   @ARTICLE{VeghLayering:2017,
   	author = {{J. V\'egh}},
   	title = {{Do we need cross layering activities or reasonable layering
   	in computing systems?}},
   	journal = {IEEE Design \& Test},
   	year = {2017},
   	pages = {submitted},
   }
   

@inbook{RenewingComputingVegh:2017,
	author = {V\'egh~J.},
	editor = {Mamta Mittal},
	series = { Advances in Parallel Computing},
	title = {{Renewing computing paradigms for more
	efficient parallelization of single-threads}}, 
	publisher = { IOS Press},
	pages = {??},
	year = {2017},
}

@ARTICLE{NeuromorphicHardwarePerformance2018,
AUTHOR={van Albada, Sacha J. and Rowley, Andrew G. and Senk, Johanna and Hopkins, Michael and Schmidt, Maximilian and Stokes, Alan B. and Lester, David R. and Diesmann, Markus and Furber, Steve B.},
TITLE={Performance Comparison of the Digital Neuromorphic Hardware SpiNNaker and the Neural Network Simulation Software NEST for a Full-Scale Cortical Microcircuit Model},
JOURNAL={Frontiers in Neuroscience},
VOLUME={12},
PAGES={291},
YEAR={2018},
URL={https://www.frontiersin.org/article/10.3389/fnins.2018.00291},
DOI={10.3389/fnins.2018.00291},
ISSN={1662-453X},
ABSTRACT={The digital neuromorphic hardware SpiNNaker has been developed with the aim of enabling
large-scale neural network simulations in real time and with low power consumption. Real-time
performance is achieved with 1 ms integration time steps, and thus applies to neural networks
for which faster time scales of the dynamics can be neglected. By slowing down the simulation,
shorter integration time steps and hence faster time scales, which are often biologically relevant,
can be incorporated. We here describe the first full-scale simulations of a cortical microcircuit
with biological time scales on SpiNNaker. Since about half the synapses onto the neurons arise
within the microcircuit, larger cortical circuits have only moderately more synapses per neuron.
Therefore, the full-scale microcircuit paves the way for simulating cortical circuits of arbitrary size.
With approximately 80,000 neurons and 0.3 billion synapses, this model is the largest simulated on
SpiNNaker to date. The scale-up is enabled by recent developments in the SpiNNaker software
stack that allow simulations to be spread across multiple boards. Comparison with simulations
using the NEST software on a high-performance cluster shows that both simulators can reach
a similar accuracy, despite the fixed-point arithmetic of SpiNNaker, demonstrating the usability
of SpiNNaker for computational neuroscience applications with biological time scales and large
network size. The runtime and power consumption are also assessed for both simulators on the
example of the cortical microcircuit model. To obtain an accuracy similar to that of NEST with
0.1 ms time steps, SpiNNaker requires a slowdown factor of around 20 compared to real time.
The runtime for NEST saturates around 3 times real time using hybrid parallelization with MPI
and multi-threading. However, achieving this runtime comes at the cost of increased power and
energy consumption. The lowest total energy consumption for NEST is reached at around 144
parallel threads and 4.6 times slowdown. At this setting, NEST and SpiNNaker have a comparable
energy consumption per synaptic event. Our results widen the application domain of SpiNNaker
and help guide its development, showing that further optimizations such as synapse-centric
network representation are necessary to enable real-time simulation of large biological neural
networks.}
}

@ARTICLE{SpiNNakerArchitecture:2013,
author={S. B. Furber and D. R. Lester and L. A. Plana and J. D. Garside and E. Painkras and S. Temple and A. D. Brown},
journal={IEEE Transactions on Computers},
title={Overview of the SpiNNaker System Architecture},
year={2013},
volume={62},
number={12},
pages={2454-2467},
abstract={SpiNNaker (a contraction of Spiking Neural Network Architecture) is a million-core computing engine whose flagship goal is to be able to simulate the behavior of aggregates of up to a billion neurons in real time. It consists of an array of ARM9 cores, communicating via packets carried by a custom interconnect fabric. The packets are small (40 or 72 bits), and their transmission is brokered entirely by hardware, giving the overall engine an extremely high bisection bandwidth of over 5 billion packets/s. Three of the principal axioms of parallel machine design (memory coherence, synchronicity, and determinism) have been discarded in the design without, surprisingly, compromising the ability to perform meaningful computations. A further attribute of the system is the acknowledgment, from the initial design stages, that the sheer size of the implementation will make component failures an inevitable aspect of day-to-day operation, and fault detection and recovery mechanisms have been built into the system at many levels of abstraction. This paper describes the architecture of the machine and outlines the underlying design philosophy; software and applications are to be described in detail elsewhere, and only introduced in passing here as necessary to illuminate the description.},
keywords={failure analysis;microprocessor chips;neural net architecture;parallel machines;synchronisation;ARM9 cores;SpiNNaker system architecture;billion neurons;bisection bandwidth;component failures;custom interconnect fabric;day-to-day operation;design philosophy;determinism;fault detection;flagship goal;memory coherence;million-core computing engine;parallel machine design;principal axioms;recovery mechanisms;spiking neural network architecture;synchronicity;Biological system modeling;Computer architecture;Network architecture;Neural networks;Program processors;Interconnection architectures;neurocomputers;parallel processors;real-time distributed},
doi={10.1109/TC.2012.142},
ISSN={0018-9340},
month={Dec},}

@Article{SpikingNeuralAccuracy:Henker2012,
author="Henker, Stephan
and Partzsch, Johannes
and Sch{\"u}ffny, Ren{\'e}",
title="Accuracy evaluation of numerical methods used in state-of-the-art simulators for spiking neural networks",
journal="Journal of Computational Neuroscience",
year="2012",
month="Apr",
day="01",
volume="32",
number="2",
pages="309--326",
abstract="With the various simulators for spiking neural networks developed in recent years, a variety of numerical solution methods for the underlying differential equations are available. In this article, we introduce an approach to systematically assess the accuracy of these methods. In contrast to previous investigations, our approach focuses on a completely deterministic comparison and uses an analytically solved model as a reference. This enables the identification of typical sources of numerical inaccuracies in state-of-the-art simulation methods. In particular, with our approach we can separate the error of the numerical integration from the timing error of spike detection and propagation, the latter being prominent in simulations with fixed timestep. To verify the correctness of the testing procedure, we relate the numerical deviations to theoretical predictions for the employed numerical methods. Finally, we give an example of the influence of simulation artefacts on network behaviour and spike-timing-dependent plasticity (STDP), underlining the importance of spike-time accuracy for the simulation of STDP.",
issn="1573-6873",
doi="10.1007/s10827-011-0353-9",
url="https://doi.org/10.1007/s10827-011-0353-9"
}

@article{NEST_2017,
title={NEST 2.14.0}, DOI={10.5281/zenodo.882971},
abstractNote={<p>NEST is a simulator for spiking neural network models that focuses on the dynamics,
size and structure of neural systems rather than on the exact morphology of individual neurons
For further information, visit http://www.nest-simulator.org.</p>
<p>The release notes for this release are available at https://github.com/nest/nest-simulator/releases/tag/v2.14.0</p>},
publisher={Zenodo},
author={Peyser, Alexander and Sinha, Ankur and Vennemo, Stine Brekke and Ippen, Tammo and Jordan, Jakob and Graber,
Steffen and Morrison, Abigail and Trensch, Guido and Fardet, Tanguy and Mørk, Håkon and et al.},
year={2017}, month={Oct}
}

@article{LimitsOfLimits2014,
        author = {Markov, I.L.},
        title = {{Limits on fundamental limits to computation}},
        journal = {Nature},
        volume = {512(7513)},
        year = {2014},
        pages = {147-154},
}


@article{VeghSupercomp:2018,
  author    = {J{\'{a}}nos V{\'{e}}gh},
  title     = {{How Amdahl's law restricts supercomputer applications and building
               ever bigger supercomputers}},
  journal   = {CoRR, in review in Parallel Computing},
  volume    = {abs/1708.01462},
  year      = {2018},
  url       = {http://arxiv.org/abs/1708.01462},
  archivePrefix = {arXiv},
  eprint    = {1708.01462},
  timestamp = {Tue, 05 Sep 2017 10:03:46 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/abs-1708-01462},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
